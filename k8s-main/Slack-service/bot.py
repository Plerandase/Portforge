"""
Slack Monitoring Bot - Socket Mode
MSA ì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì „ìš© ë´‡
"""
import os
import logging
import time
import re
import schedule
import boto3
import json
from kubernetes import client, config
from slack_bolt import App
from slack_bolt.adapter.socket_mode import SocketModeHandler
import requests
from datetime import datetime, timedelta

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.DEBUG,  # DEBUG ë ˆë²¨ë¡œ ë³€ê²½
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Slack SDK ë¡œê¹…ë„ í™œì„±í™”
logging.getLogger("slack_bolt").setLevel(logging.DEBUG)

# AWS Secret Managerì—ì„œ ì‹œí¬ë¦¿ ê°€ì ¸ì˜¤ê¸°
def get_secrets():
    try:
        client = boto3.client('secretsmanager', region_name='ap-northeast-2')
        response = client.get_secret_value(SecretId='portforge/slack-bot/all-secrets')
        secrets = json.loads(response['SecretString'])
        logger.info("âœ… AWS Secret Managerì—ì„œ ì‹œí¬ë¦¿ ë¡œë“œ ì™„ë£Œ")
        return secrets
    except Exception as e:
        logger.error(f"âŒ Secret Manager ì—ëŸ¬: {e}")
        # í™˜ê²½ë³€ìˆ˜ fallback
        return {
            'SLACK_BOT_TOKEN': os.environ.get("SLACK_BOT_TOKEN"),
            'SLACK_APP_TOKEN': os.environ.get("SLACK_APP_TOKEN"),
            'AWS_ACCESS_KEY_ID': os.environ.get("AWS_ACCESS_KEY_ID"),
            'AWS_SECRET_ACCESS_KEY': os.environ.get("AWS_SECRET_ACCESS_KEY")
        }

# ì‹œí¬ë¦¿ ë¡œë“œ
secrets = get_secrets()

# Slack ì•± ì´ˆê¸°í™” (Socket Mode)
app = App(token=secrets['SLACK_BOT_TOKEN'])

# Kubernetes í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (Pod ë¡œê·¸ ìˆ˜ì§‘ìš©)
try:
    config.load_incluster_config()  # Pod ë‚´ë¶€ì—ì„œ ì‹¤í–‰ ì‹œ
    k8s_v1 = client.CoreV1Api()
    K8S_ENABLED = True
    logger.info("âœ… Kubernetes ë¡œê·¸ ìˆ˜ì§‘ ê¸°ëŠ¥ í™œì„±í™”")
except Exception as e:
    k8s_v1 = None
    K8S_ENABLED = False
    logger.warning(f"âš ï¸ Kubernetes ë¡œê·¸ ìˆ˜ì§‘ ë¹„í™œì„±í™”: {e}")

# Bedrock í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (AI ì—ëŸ¬ ë¶„ì„ìš©)
try:
    bedrock_client = boto3.client(
        'bedrock-runtime',
        region_name=os.environ.get("BEDROCK_REGION", "us-east-1"),
        aws_access_key_id=secrets['AWS_ACCESS_KEY_ID'],
        aws_secret_access_key=secrets['AWS_SECRET_ACCESS_KEY']
    )
    AI_ANALYSIS_ENABLED = True
    logger.info("âœ… Bedrock AI ë¶„ì„ ê¸°ëŠ¥ í™œì„±í™”")
except Exception as e:
    bedrock_client = None
    AI_ANALYSIS_ENABLED = False
    logger.warning(f"âš ï¸ Bedrock AI ë¶„ì„ ë¹„í™œì„±í™”: {e}")

# ì•Œë¦¼ ì±„ë„ ì„¤ì •
ALERT_CHANNEL = os.environ.get("ALERT_CHANNEL", "#alerts")

# MSA ì„œë¹„ìŠ¤ URL (Kubernetes Service Discovery)
SERVICES = {
    "project": {
        "url": os.environ.get("PROJECT_SERVICE_URL", "http://project-service:8001"),
        "name": "Project ì„œë¹„ìŠ¤",
        "k8s_deployment": "project-service"  # ì‹¤ì œ deployment ì´ë¦„
    },
    "team": {
        "url": os.environ.get("TEAM_SERVICE_URL", "http://team-service:8002"),
        "name": "Team ì„œë¹„ìŠ¤",
        "k8s_deployment": "team-service"  # ì‹¤ì œ deployment ì´ë¦„
    },
    "ai": {
        "url": os.environ.get("AI_SERVICE_URL", "http://ai-service:8003"),
        "name": "AI ì„œë¹„ìŠ¤",
        "k8s_deployment": "ai-service"  # ì‹¤ì œ deployment ì´ë¦„
    },
    "auth": {
        "url": os.environ.get("AUTH_SERVICE_URL", "http://auth-service:8000"),
        "name": "Auth ì„œë¹„ìŠ¤",
        "k8s_deployment": "auth-deployment"  # ì‹¤ì œ deployment ì´ë¦„ (í•˜ë“œì½”ë”©)
    },
    "support": {
        "url": os.environ.get("SUPPORT_SERVICE_URL", "http://support-service:8004"),
        "name": "Support ì„œë¹„ìŠ¤",
        "k8s_deployment": "support-deployment"  # ì‹¤ì œ deployment ì´ë¦„ (í•˜ë“œì½”ë”©)
    }
}

# ì„ê³„ê°’ ì„¤ì •
CPU_THRESHOLD = int(os.environ.get("CPU_THRESHOLD", "80"))
MEMORY_THRESHOLD_PCT = int(os.environ.get("MEMORY_THRESHOLD", "80"))  # í™˜ê²½ë³€ìˆ˜ëª… ìˆ˜ì •
RESPONSE_TIME_THRESHOLD = int(os.environ.get("RESPONSE_TIME_THRESHOLD", "100"))  # 100ms
ERROR_RATE_THRESHOLD = int(os.environ.get("ERROR_RATE_THRESHOLD", "20"))  # 20%ë¡œ ì„¤ì •

# Kubernetes ë©”ëª¨ë¦¬ ì œí•œ (Mi ë‹¨ìœ„)
MEMORY_LIMITS = {
    "auth": 512,      # 512Mi
    "ai": 1024,       # 1Gi
    "team": 1024,     # 1Gi  
    "project": 1024,  # 1Gi
    "support": 512    # ê¸°ë³¸ê°’ (deploymentì— ì œí•œ ì—†ìŒ)
}


# ===== ëª¨ë‹ˆí„°ë§ í•¨ìˆ˜ =====

def check_service_health(service_key: str, service_info: dict) -> dict:
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"""
    try:
        url = f"{service_info['url']}/health"
        response = requests.get(url, timeout=5)
        
        return {
            "service": service_info['name'],
            "status": "healthy" if response.status_code == 200 else "unhealthy",
            "status_code": response.status_code,
            "response_time": response.elapsed.total_seconds()
        }
    except requests.exceptions.RequestException as e:
        logger.error(f"{service_info['name']} health check failed: {e}")
        return {
            "service": service_info['name'],
            "status": "down",
            "error": str(e)
        }


def get_memory_usage_percentage(service_key: str, current_mb: float) -> float:
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ í¼ì„¼í‹°ì§€ë¡œ ê³„ì‚°"""
    limit_mb = MEMORY_LIMITS.get(service_key, 512)
    return round((current_mb / limit_mb) * 100, 1)


def format_error_rate_with_calculation(metrics_text: str, error_rate: float, total_requests: int) -> str:
    """ì—ëŸ¬ìœ¨ì„ ê³„ì‚°ì‹ê³¼ í•¨ê»˜ í‘œì‹œ (5xxë§Œ ì—ëŸ¬ë¡œ ê³„ì‚°)"""
    if total_requests == 0:
        return "ì—ëŸ¬ìœ¨: 0% (ìš”ì²­ ì—†ìŒ)"
    
    # 4xx, 5xx ìš”ì²­ ìˆ˜ ê°œë³„ ì§‘ê³„
    error_4xx = 0
    error_5xx = 0
    success_2xx = 0
    
    http_requests = re.findall(r'http_requests_total\{[^}]*status="([^"]+)"[^}]*\}\s+([\d.]+)', metrics_text)
    for status, count in http_requests:
        count = int(float(count))
        if status == "2xx":
            success_2xx += count
        elif status == "4xx":
            error_4xx += count
        elif status == "5xx":
            error_5xx += count
    
    # 5xxë§Œ ì—ëŸ¬ë¡œ ê³„ì‚°
    error_total = error_5xx
    
    # ìƒì„¸ ê³„ì‚°ì‹ í¬í•¨ ë¬¸ìì—´ ìƒì„±
    return (f"ì—ëŸ¬ìœ¨: {error_rate}% (ì„œë²„ì—ëŸ¬ {error_total}ê±´ / ì „ì²´ {total_requests}ê±´)\n"
            f"    â”” 5xx: {error_5xx}ê±´, 4xx: {error_4xx}ê±´ (í´ë¼ì´ì–¸íŠ¸ì—ëŸ¬), 2xx: {success_2xx}ê±´")


def check_service_metrics(service_key: str, service_info: dict) -> dict:
    """ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ (Prometheus í˜•ì‹ íŒŒì‹±)"""
    try:
        url = f"{service_info['url']}/metrics"
        response = requests.get(url, timeout=5)
        
        if response.status_code == 200:
            metrics_text = response.text
            parsed_metrics = parse_prometheus_metrics(metrics_text)
            
            return {
                "service": service_info['name'],
                "memory_usage_mb": parsed_metrics.get("memory_usage_mb", 0),
                "cpu_seconds_total": parsed_metrics.get("cpu_seconds_total", 0),
                "error_rate": parsed_metrics.get("error_rate", 0),
                "request_count": parsed_metrics.get("request_count", 0),
                "avg_response_time": parsed_metrics.get("avg_response_time", 0),
                "open_file_descriptors": parsed_metrics.get("open_fds", 0),
                "raw_metrics_text": metrics_text  # ì—ëŸ¬ìœ¨ ìƒì„¸ ë¶„ì„ìš©
            }
    except Exception as e:
        logger.error(f"{service_info['name']} metrics check failed: {e}")
    
    return None


def parse_prometheus_metrics(metrics_text: str) -> dict:
    """Prometheus í…ìŠ¤íŠ¸ í˜•ì‹ì„ íŒŒì‹±í•˜ì—¬ ì£¼ìš” ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
    metrics = {}
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë°”ì´íŠ¸ â†’ MB ë³€í™˜)
    memory_match = re.search(r'process_resident_memory_bytes\s+([\d.e+]+)', metrics_text)
    if memory_match:
        memory_bytes = float(memory_match.group(1))
        metrics["memory_usage_mb"] = round(memory_bytes / 1024 / 1024, 1)
    
    # CPU ì‚¬ìš© ì‹œê°„ (ì´ ëˆ„ì  ì‹œê°„)
    cpu_match = re.search(r'process_cpu_seconds_total\s+([\d.]+)', metrics_text)
    if cpu_match:
        metrics["cpu_seconds_total"] = float(cpu_match.group(1))
    
    # ì—´ë¦° íŒŒì¼ ë””ìŠ¤í¬ë¦½í„°
    fds_match = re.search(r'process_open_fds\s+([\d.]+)', metrics_text)
    if fds_match:
        metrics["open_fds"] = int(float(fds_match.group(1)))
    
    # HTTP ìš”ì²­ í†µê³„ë¡œ ì—ëŸ¬ìœ¨ ê³„ì‚° (5xxë§Œ ì—ëŸ¬ë¡œ ê³„ì‚°)
    total_requests = 0
    error_requests = 0
    
    # ëª¨ë“  HTTP ìš”ì²­ ìˆ˜ì§‘
    http_requests = re.findall(r'http_requests_total\{[^}]*status="([^"]+)"[^}]*\}\s+([\d.]+)', metrics_text)
    for status, count in http_requests:
        count = float(count)
        total_requests += count
        if status in ["5xx"]:  # 5xxë§Œ ì—ëŸ¬ë¡œ ê³„ì‚° (ì„œë²„ ì—ëŸ¬ë§Œ)
            error_requests += count
    
    if total_requests > 0:
        metrics["error_rate"] = round((error_requests / total_requests) * 100, 2)
        metrics["request_count"] = int(total_requests)
    
    # í‰ê·  ì‘ë‹µ ì‹œê°„ ê³„ì‚° (ì „ì²´ ìš”ì²­ ê¸°ì¤€)
    duration_sum_match = re.search(r'http_request_duration_histogram_seconds_sum\s+([\d.]+)', metrics_text)
    duration_count_match = re.search(r'http_request_duration_histogram_seconds_count\s+([\d.]+)', metrics_text)
    
    if duration_sum_match and duration_count_match:
        duration_sum = float(duration_sum_match.group(1))
        duration_count = float(duration_count_match.group(1))
        if duration_count > 0:
            metrics["avg_response_time"] = round((duration_sum / duration_count) * 1000, 2)  # ms ë‹¨ìœ„
    
    return metrics


def analyze_error_patterns(service_key: str, metrics_text: str) -> dict:
    """ì—ëŸ¬ íŒ¨í„´ ë¶„ì„ ë° ìƒì„¸ ì •ë³´ ìˆ˜ì§‘"""
    error_analysis = {
        "4xx_endpoints": [],  # 4xx ì—ëŸ¬ê°€ ë§ì€ ì—”ë“œí¬ì¸íŠ¸
        "5xx_endpoints": [],  # 5xx ì—ëŸ¬ê°€ ë§ì€ ì—”ë“œí¬ì¸íŠ¸
        "high_latency_endpoints": [],  # ì‘ë‹µì‹œê°„ì´ ê¸´ ì—”ë“œí¬ì¸íŠ¸
        "memory_usage": 0,
        "cpu_usage": 0,
        "total_requests": 0,
        "error_rate": 0
    }
    
    # Prometheus ë©”íŠ¸ë¦­ì—ì„œ ì—”ë“œí¬ì¸íŠ¸ë³„ ì—ëŸ¬ ì¶”ì¶œ
    # http_requests_total{method="POST", status="4xx", endpoint="/auth/verify-email"} 15
    endpoint_errors = re.findall(r'http_requests_total\{[^}]*endpoint="([^"]+)"[^}]*status="([^"]+)"[^}]*\}\s+([\d.]+)', metrics_text)
    
    for endpoint, status, count in endpoint_errors:
        count = int(float(count))
        if status == "4xx" and count > 5:  # 4xx ì—ëŸ¬ê°€ 5ê±´ ì´ìƒ
            error_analysis["4xx_endpoints"].append({
                "endpoint": endpoint,
                "count": count
            })
        elif status == "5xx" and count > 0:  # 5xx ì—ëŸ¬ê°€ 1ê±´ ì´ìƒ
            error_analysis["5xx_endpoints"].append({
                "endpoint": endpoint, 
                "count": count
            })
    
    # ì‘ë‹µì‹œê°„ ë¶„ì„ (ì—”ë“œí¬ì¸íŠ¸ë³„)
    duration_metrics = re.findall(r'http_request_duration_histogram_seconds_sum\{[^}]*endpoint="([^"]+)"[^}]*\}\s+([\d.]+)', metrics_text)
    duration_counts = re.findall(r'http_request_duration_histogram_seconds_count\{[^}]*endpoint="([^"]+)"[^}]*\}\s+([\d.]+)', metrics_text)
    
    # ì—”ë“œí¬ì¸íŠ¸ë³„ í‰ê·  ì‘ë‹µì‹œê°„ ê³„ì‚°
    duration_dict = {endpoint: float(duration) for endpoint, duration in duration_metrics}
    count_dict = {endpoint: float(count) for endpoint, count in duration_counts}
    
    for endpoint in duration_dict:
        if endpoint in count_dict and count_dict[endpoint] > 0:
            avg_duration = (duration_dict[endpoint] / count_dict[endpoint]) * 1000  # ms ë‹¨ìœ„
            if avg_duration > 500:  # 500ms ì´ìƒ
                error_analysis["high_latency_endpoints"].append({
                    "endpoint": endpoint,
                    "latency_ms": round(avg_duration, 2)
                })
    
    return error_analysis


def collect_recent_logs(service_key: str, service_info: dict, minutes: int = 10) -> list:
    """Kubernetes APIë¥¼ í†µí•´ ìµœê·¼ ë¡œê·¸ ìˆ˜ì§‘"""
    if not K8S_ENABLED:
        return []
    
    try:
        deployment_name = service_info.get("k8s_deployment")
        namespace = os.environ.get("K8S_NAMESPACE", "default")
        
        logger.info(f"ë¡œê·¸ ìˆ˜ì§‘ ì‹œë„: {deployment_name} in namespace {namespace}")
        
        # ì„œë¹„ìŠ¤ë³„ ë¼ë²¨ ì…€ë ‰í„° ë§¤í•‘ (í•˜ë“œì½”ë”©)
        label_selectors = {
            "auth": "app=auth-service",           # auth-deploymentì˜ ë¼ë²¨
            "support": "app=support",             # support-deploymentì˜ ë¼ë²¨ (ìˆ˜ì •ë¨)
            "ai": "app=ai-service",               # ai-serviceì˜ ë¼ë²¨
            "team": "app=team-service",           # team-serviceì˜ ë¼ë²¨
            "project": "app=project-service"      # project-serviceì˜ ë¼ë²¨
        }
        
        label_selector = label_selectors.get(service_key, f"app={deployment_name}")
        
        # Deploymentì˜ Pod ëª©ë¡ ì¡°íšŒ
        pods = k8s_v1.list_namespaced_pod(
            namespace=namespace,
            label_selector=label_selector
        )
        
        if not pods.items:
            logger.warning(f"No pods found for {deployment_name} with label {label_selector}")
            return []
        
        # ìµœì‹  Podì˜ ë¡œê·¸ ìˆ˜ì§‘
        pod_name = pods.items[0].metadata.name
        since_seconds = minutes * 60
        
        logger.info(f"Pod ë¡œê·¸ ìˆ˜ì§‘: {pod_name} (ìµœê·¼ {minutes}ë¶„)")
        
        # Pod ë¡œê·¸ ì¡°íšŒ
        log_response = k8s_v1.read_namespaced_pod_log(
            name=pod_name,
            namespace=namespace,
            since_seconds=since_seconds,
            tail_lines=50  # ìµœê·¼ 50ì¤„ë§Œ
        )
        
        # ì—ëŸ¬ ë¡œê·¸ë§Œ í•„í„°ë§
        error_logs = []
        for line in log_response.split('\n'):
            if any(keyword in line.lower() for keyword in ['error', 'exception', 'failed', 'timeout', 'refused', 'denied']):
                error_logs.append(line.strip())
        
        logger.info(f"ì—ëŸ¬ ë¡œê·¸ {len(error_logs)}ê°œ ìˆ˜ì§‘ë¨")
        return error_logs[-10:]  # ìµœê·¼ 10ê°œ ì—ëŸ¬ë§Œ
        
    except Exception as e:
        logger.error(f"ë¡œê·¸ ìˆ˜ì§‘ ì‹¤íŒ¨ ({service_key}): {e}")
        return []


def analyze_errors_with_enhanced_context(service_key: str, error_analysis: dict, metrics: dict) -> str:
    """ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í–¥ìƒëœ AI ë¶„ì„"""
    if not AI_ANALYSIS_ENABLED:
        return "AI ë¶„ì„ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
    
    # ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ìˆ˜ì§‘
    recent_logs = collect_recent_logs(service_key, SERVICES[service_key])
    
    # ì„œë¹„ìŠ¤ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    service_context = {
        "auth": "Auth ì„œë¹„ìŠ¤ - JWT í† í°, ë¡œê·¸ì¸, íšŒì›ê°€ì… ì²˜ë¦¬",
        "ai": "AI ì„œë¹„ìŠ¤ - Bedrock í˜¸ì¶œ, í…ŒìŠ¤íŠ¸ ìƒì„±, íšŒì˜ë¡ ì‘ì„±",
        "project": "Project ì„œë¹„ìŠ¤ - í”„ë¡œì íŠ¸ CRUD, ì§€ì›ì„œ ê´€ë¦¬",
        "team": "Team ì„œë¹„ìŠ¤ - íŒ€ ìƒì„±, ë©¤ë²„ ê´€ë¦¬, íŒŒì¼ ì—…ë¡œë“œ",
        "support": "Support ì„œë¹„ìŠ¤ - ê³µì§€ì‚¬í•­, ì´ë²¤íŠ¸, ì±„íŒ…"
    }
    
    # ì‹¤ì œ deployment ì´ë¦„ ë§¤í•‘
    deployment_names = {
        "auth": "auth-deployment",
        "support": "support-deployment",
        "ai": "ai-service",
        "team": "team-service", 
        "project": "project-service"
    }
    
    context = service_context.get(service_key, f"{service_key} ì„œë¹„ìŠ¤")
    deployment_name = deployment_names.get(service_key, f"{service_key}-service")
    
    # ì—ëŸ¬ìœ¨ ì„ê³„ê°’ ì²´í¬
    error_rate = metrics.get('error_rate', 0)
    error_status = "ì •ìƒ" if error_rate <= ERROR_RATE_THRESHOLD else f"ì„ê³„ê°’ ì´ˆê³¼ (ê¸°ì¤€: {ERROR_RATE_THRESHOLD}%)"
    
    prompt = f"""ë‹¹ì‹ ì€ MSA ì„œë¹„ìŠ¤ ì¥ì•  ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì„œë¹„ìŠ¤: {context}
Kubernetes Deployment: {deployment_name}

í˜„ì¬ ë©”íŠ¸ë¦­:
â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {metrics.get('memory_usage_mb', 0)}MB
â€¢ ì „ì²´ ìš”ì²­ ìˆ˜: {metrics.get('request_count', 0):,}ê±´
â€¢ ì—ëŸ¬ìœ¨: {error_rate}% ({error_status})
â€¢ í‰ê·  ì‘ë‹µì‹œê°„: {metrics.get('avg_response_time', 0)}ms
â€¢ ì—´ë¦° íŒŒì¼: {metrics.get('open_file_descriptors', 0)}ê°œ

ë¬¸ì œ ì—”ë“œí¬ì¸íŠ¸:
4xx ì—ëŸ¬: {error_analysis.get('4xx_endpoints', [])}
5xx ì—ëŸ¬: {error_analysis.get('5xx_endpoints', [])}
ë†’ì€ ì§€ì—°ì‹œê°„: {error_analysis.get('high_latency_endpoints', [])}

ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (10ë¶„ ë‚´):
{chr(10).join(recent_logs) if recent_logs else "ì—ëŸ¬ ë¡œê·¸ ì—†ìŒ"}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš” (ë§ˆí¬ë‹¤ìš´ êµµì€ ê¸€ì”¨ ì‚¬ìš© ê¸ˆì§€):

1. ì¦‰ì‹œ í™•ì¸ì‚¬í•­ (5ë¶„ ë‚´ ì²´í¬ ê°€ëŠ¥í•œ kubectl ëª…ë ¹ì–´)
   - kubectl logs deployment/{deployment_name} --tail=100 | grep ERROR
   - kubectl get pods -l app={service_key}-service
   - kubectl describe deployment {deployment_name}

2. ë¡œê·¸ ê¸°ë°˜ ì›ì¸ ë¶„ì„
3. ìš°ì„ ìˆœìœ„ë³„ ì¡°ì¹˜ (ì‹¬ê°ë„ ìˆœ, êµ¬ì²´ì  ëª…ë ¹ì–´ í¬í•¨)
4. ëª¨ë‹ˆí„°ë§ í¬ì¸íŠ¸ (ì§€ì† ê´€ì°° í•„ìš”í•œ ë©”íŠ¸ë¦­)

ë¶„ì„ ë§ˆì§€ë§‰ì— "ìœ„ì™€ ê°™ì€ ë¶„ì„ê³¼ ì¡°ì¹˜ì‚¬í•­ì„ í†µí•´..." ê°™ì€ ë¶ˆí•„ìš”í•œ ë§ˆë¬´ë¦¬ ë¬¸ì¥ì€ ì œì™¸í•´ì£¼ì„¸ìš”."""

    try:
        # Claude 3 Haiku í˜¸ì¶œ
        response = bedrock_client.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 1200,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ]
            })
        )
        
        # ì‘ë‹µ íŒŒì‹±
        response_body = json.loads(response['body'].read())
        diagnosis = response_body['content'][0]['text']
        return diagnosis
        
    except Exception as e:
        logger.error(f"Bedrock ì—ëŸ¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def send_ai_enhanced_error_alert(service_key: str, service_info: dict, metrics: dict):
    """ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ AI ë¶„ì„ ì•Œë¦¼"""
    # ì—ëŸ¬ íŒ¨í„´ ë¶„ì„
    error_analysis = analyze_error_patterns(service_key, metrics.get('raw_metrics_text', ''))
    
    # ë¡œê·¸ í¬í•¨ AI ë¶„ì„
    ai_diagnosis = analyze_errors_with_enhanced_context(service_key, error_analysis, metrics)
    
    # ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ìˆ˜ì§‘
    recent_logs = collect_recent_logs(service_key, service_info, minutes=5)
    
    # ìŠ¤ë§ˆíŠ¸ ì•Œë¦¼ ë©”ì‹œì§€ êµ¬ì„±
    alert_message = f"""ğŸš¨ {service_info['name']} ì—ëŸ¬ ë°œìƒ + AI ë¶„ì„

í˜„ì¬ ìƒí™©:
â€¢ ì—ëŸ¬ìœ¨: {metrics['error_rate']}% (ì„ê³„ê°’: {ERROR_RATE_THRESHOLD}%)
â€¢ ì´ ìš”ì²­: {metrics['request_count']:,}ê±´
â€¢ ë©”ëª¨ë¦¬: {get_memory_usage_percentage(service_key, metrics['memory_usage_mb'])}%
â€¢ í‰ê·  ì‘ë‹µì‹œê°„: {metrics['avg_response_time']}ms

ë¬¸ì œ ì—”ë“œí¬ì¸íŠ¸:"""
    
    # 5xx ì—ëŸ¬ ìƒì„¸ (ìš°ì„ ìˆœìœ„)
    if error_analysis.get('5xx_endpoints'):
        for error in error_analysis.get('5xx_endpoints', [])[:3]:  # ìƒìœ„ 3ê°œë§Œ
            alert_message += f"\nâ€¢ {error['endpoint']}: {error['count']}ê±´ (5xx)"
    
    # 4xx ì—ëŸ¬ ìƒì„¸
    if error_analysis.get('4xx_endpoints'):
        for error in error_analysis.get('4xx_endpoints', [])[:2]:  # ìƒìœ„ 2ê°œë§Œ
            alert_message += f"\nâ€¢ {error['endpoint']}: {error['count']}ê±´ (4xx)"
    
    # ë†’ì€ ì§€ì—°ì‹œê°„ ì—”ë“œí¬ì¸íŠ¸
    if error_analysis.get('high_latency_endpoints'):
        for latency in error_analysis.get('high_latency_endpoints', [])[:2]:  # ìƒìœ„ 2ê°œë§Œ
            alert_message += f"\nâ€¢ {latency['endpoint']}: {latency['latency_ms']}ms (ì§€ì—°)"
    
    if not any([error_analysis.get('5xx_endpoints'), error_analysis.get('4xx_endpoints'), error_analysis.get('high_latency_endpoints')]):
        alert_message += "\nâ€¢ ì—”ë“œí¬ì¸íŠ¸ë³„ ìƒì„¸ ì •ë³´ ì—†ìŒ"
    
    # ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (ê°„ëµíˆ)
    if recent_logs:
        alert_message += f"\n\nìµœê·¼ ì—ëŸ¬ ë¡œê·¸ (5ë¶„ ë‚´):"
        for log in recent_logs[:3]:  # ìµœê·¼ 3ê°œë§Œ
            # ë¡œê·¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            short_log = log[:100] + "..." if len(log) > 100 else log
            alert_message += f"\nâ€¢ {short_log}"
    
    # AI ì§„ë‹¨ ê²°ê³¼
    alert_message += f"""

AI ì§„ë‹¨ ê²°ê³¼:
{ai_diagnosis}

â° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"""
    
    send_alert(alert_message, severity="critical")


def send_alert(message: str, severity: str = "info"):
    """Slack ì•Œë¦¼ ì „ì†¡"""
    emoji = {
        "critical": "ğŸ”´",
        "warning": "âš ï¸",
        "info": "â„¹ï¸",
        "success": "âœ…"
    }.get(severity, "ğŸ“¢")
    
    try:
        app.client.chat_postMessage(
            channel=ALERT_CHANNEL,
            text=f"{emoji} {message}",
            unfurl_links=False,
            unfurl_media=False
        )
        logger.info(f"Alert sent: {message}")
    except Exception as e:
        logger.error(f"Failed to send alert: {e}")


# ===== ì£¼ê¸°ì  ëª¨ë‹ˆí„°ë§ ì‘ì—… =====

def monitor_resources():
    """ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥  ëª¨ë‹ˆí„°ë§ (5ë¶„ë§ˆë‹¤) - í¼ì„¼í‹°ì§€ ê¸°ë°˜"""
    logger.info("Checking resource usage...")
    
    for service_key, service_info in SERVICES.items():
        metrics = check_service_metrics(service_key, service_info)
        
        if metrics:
            # ë©”ëª¨ë¦¬ í¼ì„¼í‹°ì§€ ì²´í¬
            memory_mb = metrics["memory_usage_mb"]
            memory_pct = get_memory_usage_percentage(service_key, memory_mb)
            memory_limit = MEMORY_LIMITS.get(service_key, 512)
            
            if memory_pct > MEMORY_THRESHOLD_PCT:
                send_alert(
                    f"{metrics['service']} ë©”ëª¨ë¦¬ {memory_pct}% "
                    f"({memory_mb}MB / {memory_limit}MB) "
                    f"ì„ê³„ê°’: {MEMORY_THRESHOLD_PCT}%",
                    severity="warning"
                )
            
            # ì—ëŸ¬ìœ¨ ì²´í¬ - AI ë¶„ì„ í¬í•¨
            if metrics["error_rate"] > ERROR_RATE_THRESHOLD:
                if AI_ANALYSIS_ENABLED:
                    send_ai_enhanced_error_alert(service_key, service_info, metrics)
                else:
                    # ê¸°ì¡´ ì•Œë¦¼ (AI ì—†ì´)
                    error_detail = format_error_rate_with_calculation(
                        metrics.get('raw_metrics_text', ''), 
                        metrics['error_rate'], 
                        metrics['request_count']
                    )
                    send_alert(
                        f"{metrics['service']} ì—ëŸ¬ìœ¨ {metrics['error_rate']}% "
                        f"(ì„ê³„ê°’: {ERROR_RATE_THRESHOLD}%)\n{error_detail}",
                        severity="critical"
                    )
            
            # ì‘ë‹µ ì‹œê°„ ì²´í¬
            if metrics["avg_response_time"] > RESPONSE_TIME_THRESHOLD:
                send_alert(
                    f"{metrics['service']} í‰ê·  ì‘ë‹µì‹œê°„ {metrics['avg_response_time']}ms "
                    f"(ì„ê³„ê°’: {RESPONSE_TIME_THRESHOLD}ms)",
                    severity="warning"
                )


def monitor_health():
    """ì„œë¹„ìŠ¤ í—¬ìŠ¤ ëª¨ë‹ˆí„°ë§ (1ë¶„ë§ˆë‹¤)"""
    logger.info("Checking service health...")
    
    for service_key, service_info in SERVICES.items():
        health = check_service_health(service_key, service_info)
        
        if health["status"] == "down":
            send_alert(
                f"{health['service']} ì„œë¹„ìŠ¤ ë‹¤ìš´! ì—ëŸ¬: {health.get('error', 'Unknown')}",
                severity="critical"
            )
        elif health["status"] == "unhealthy":
            send_alert(
                f"{health['service']} ì„œë¹„ìŠ¤ ë¹„ì •ìƒ (HTTP {health['status_code']})",
                severity="warning"
            )


# ===== Slack ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ =====

@app.event("app_mention")
def handle_mention(event, say):
    """ë´‡ ë©˜ì…˜ ì²˜ë¦¬"""
    logger.info(f"ğŸ¯ ë©˜ì…˜ ì´ë²¤íŠ¸ ìˆ˜ì‹ : {event}")
    try:
        text = event.get("text", "").lower()
        logger.info(f"ğŸ“ ìˆ˜ì‹ ëœ í…ìŠ¤íŠ¸: {text}")
        
        if "ìƒíƒœ" in text or "status" in text:
            logger.info("ìƒíƒœ ëª…ë ¹ì–´ ì²˜ë¦¬ ì¤‘...")
            response = get_all_status()
        elif "í—¬í”„" in text or "help" in text:
            logger.info("ë„ì›€ë§ ëª…ë ¹ì–´ ì²˜ë¦¬ ì¤‘...")
            response = get_help_message()
        elif "ë©”íŠ¸ë¦­" in text or "metrics" in text:
            logger.info("ë©”íŠ¸ë¦­ ëª…ë ¹ì–´ ì²˜ë¦¬ ì¤‘...")
            response = get_all_metrics()
        else:
            logger.info("ê¸°ë³¸ ì‘ë‹µ ì²˜ë¦¬ ì¤‘...")
            response = "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? 'help'ë¥¼ ì…ë ¥í•˜ë©´ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        logger.info(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡ ì¤‘: {len(response)} ë¬¸ì")
        say(response)
        logger.info("âœ… ì‘ë‹µ ì „ì†¡ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë©˜ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        say(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


def get_all_status() -> str:
    """ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ"""
    result = f"ğŸ“Š *ì„œë¹„ìŠ¤ ìƒíƒœ ë¦¬í¬íŠ¸* ({datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n\n"
    
    for service_key, service_info in SERVICES.items():
        health = check_service_health(service_key, service_info)
        
        status_emoji = {
            "healthy": "âœ…",
            "unhealthy": "âš ï¸",
            "down": "ğŸ”´"
        }.get(health["status"], "â“")
        
        result += f"{status_emoji} *{health['service']}*\n"
        result += f"  ìƒíƒœ: {health['status']}\n"
        
        if health["status"] == "healthy":
            result += f"  ì‘ë‹µì‹œê°„: {health['response_time']:.2f}ì´ˆ\n"
        elif "error" in health:
            result += f"  ì—ëŸ¬: {health['error']}\n"
        
        result += "\n"
    
    return result


def get_all_metrics() -> str:
    """ëª¨ë“  ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ (í–¥ìƒëœ ë²„ì „ + ì‹œê°„ ì •ë³´)"""
    current_time = datetime.now()
    result = f"ğŸ“ˆ *ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ë¦¬í¬íŠ¸* ({current_time.strftime('%Y-%m-%d %H:%M:%S')})\n\n"
    
    for service_key, service_info in SERVICES.items():
        metrics = check_service_metrics(service_key, service_info)
        
        if metrics:
            # ë©”ëª¨ë¦¬ í¼ì„¼í‹°ì§€ ê³„ì‚°
            memory_mb = metrics['memory_usage_mb']
            memory_pct = get_memory_usage_percentage(service_key, memory_mb)
            memory_limit = MEMORY_LIMITS.get(service_key, 512)
            
            # ì—ëŸ¬ìœ¨ ìƒì„¸ ì •ë³´
            error_detail = format_error_rate_with_calculation(
                metrics.get('raw_metrics_text', ''), 
                metrics['error_rate'], 
                metrics['request_count']
            )
            
            # ìƒíƒœ ì•„ì´ì½˜ ê²°ì •
            status_icon = "ğŸ”´" if memory_pct > 90 else "âš ï¸" if memory_pct > 80 else "âœ…"
            
            # CPU ì‚¬ìš©ë¥  ì¶”ì • (ëˆ„ì  ì‹œê°„ ê¸°ë°˜)
            cpu_total = metrics['cpu_seconds_total']
            uptime_estimate = "ì•Œ ìˆ˜ ì—†ìŒ"
            if cpu_total > 0:
                # ëŒ€ëµì ì¸ ì—…íƒ€ì„ ì¶”ì • (CPU ì‹œê°„ / ì½”ì–´ ìˆ˜)
                estimated_uptime_hours = cpu_total / 0.5  # 0.5 ì½”ì–´ ê¸°ì¤€
                if estimated_uptime_hours < 24:
                    uptime_estimate = f"{estimated_uptime_hours:.1f}ì‹œê°„"
                else:
                    uptime_estimate = f"{estimated_uptime_hours/24:.1f}ì¼"
            
            result += f"*{metrics['service']}*\n"
            result += f"  ë©”ëª¨ë¦¬: {memory_mb}MB ({memory_pct}% / {memory_limit}MB ì œí•œ)\n"
            result += f"  {error_detail}\n"
            result += f"  ìš”ì²­ ìˆ˜: {metrics['request_count']:,} (ì„œë¹„ìŠ¤ ì‹œì‘ë¶€í„° ëˆ„ì )\n"
            result += f"  í‰ê·  ì‘ë‹µì‹œê°„: {metrics['avg_response_time']}ms\n"
            result += f"  ì—´ë¦° íŒŒì¼: {metrics['open_file_descriptors']}\n"
            result += f"  CPU ëˆ„ì ì‹œê°„: {metrics['cpu_seconds_total']}ì´ˆ (ì¶”ì • ì—…íƒ€ì„: {uptime_estimate})\n"
            result += f"  ìƒíƒœ: {status_icon}\n\n"
        else:
            result += f"*{service_info['name']}*\n"
            result += f"  ë©”íŠ¸ë¦­ ì¡°íšŒ ì‹¤íŒ¨ âŒ\n\n"
    
    result += f"\nğŸ’¡ *ì°¸ê³ ì‚¬í•­:*\n"
    result += f"â€¢ ìš”ì²­ ìˆ˜ëŠ” ì„œë¹„ìŠ¤ ì‹œì‘ë¶€í„°ì˜ ëˆ„ì ê°’ì…ë‹ˆë‹¤\n"
    result += f"â€¢ Pod ì¬ì‹œì‘ ì‹œ ì¹´ìš´í„°ê°€ 0ìœ¼ë¡œ ë¦¬ì…‹ë©ë‹ˆë‹¤\n"
    result += f"â€¢ í˜„ì¬ ì‹œì  ê¸°ì¤€ ìŠ¤ëƒ…ìƒ· ë°ì´í„°ì…ë‹ˆë‹¤ (íˆìŠ¤í† ë¦¬ ì—†ìŒ)\n"
    result += f"â€¢ ë” ì •í™•í•œ íŠ¸ë Œë“œ ë¶„ì„ì€ Grafanaë¥¼ ì´ìš©í•˜ì„¸ìš”: https://grafana.portforge.org"
    
    return result


def get_help_message() -> str:
    """ë„ì›€ë§"""
    return f"""
ğŸ¤– *PortForge ëª¨ë‹ˆí„°ë§ ë´‡*

*ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´:*

ğŸ“Š ìƒíƒœ í™•ì¸:
  â€¢ @bot ìƒíƒœ - ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ
  â€¢ @bot status - ëª¨ë“  ì„œë¹„ìŠ¤ ìƒíƒœ ì¡°íšŒ

ğŸ“ˆ ë©”íŠ¸ë¦­ í™•ì¸:
  â€¢ @bot ë©”íŠ¸ë¦­ - ëª¨ë“  ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ (í¼ì„¼í‹°ì§€ + ìƒì„¸ ì—ëŸ¬ìœ¨)
  â€¢ @bot metrics - ëª¨ë“  ì„œë¹„ìŠ¤ ë©”íŠ¸ë¦­ ì¡°íšŒ

â“ ë„ì›€ë§:
  â€¢ @bot help - ì´ ë„ì›€ë§ í‘œì‹œ

*ìë™ ì•Œë¦¼ ì„ê³„ê°’:*
â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {MEMORY_THRESHOLD_PCT}% (Kubernetes ì œí•œ ê¸°ì¤€)
â€¢ ì‘ë‹µì‹œê°„: {RESPONSE_TIME_THRESHOLD}ms ì´ˆê³¼ ì‹œ ìë™ ì•Œë¦¼
â€¢ ì—ëŸ¬ìœ¨: {ERROR_RATE_THRESHOLD}% ì´ˆê³¼ ì‹œ ìë™ ì•Œë¦¼ (5xx ì„œë²„ì—ëŸ¬ë§Œ)
â€¢ ì„œë¹„ìŠ¤ ë‹¤ìš´ ì‹œ ì¦‰ì‹œ ì•Œë¦¼

*ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •:*
â€¢ Auth: {MEMORY_LIMITS['auth']}MB
â€¢ AI: {MEMORY_LIMITS['ai']}MB  
â€¢ Team: {MEMORY_LIMITS['team']}MB
â€¢ Project: {MEMORY_LIMITS['project']}MB
â€¢ Support: {MEMORY_LIMITS['support']}MB

*ì•Œë¦¼ ì±„ë„:* {ALERT_CHANNEL}

*AI ë¶„ì„:* {"âœ… í™œì„±í™” (Bedrock)" if AI_ANALYSIS_ENABLED else "âŒ ë¹„í™œì„±í™”"}
*ë¡œê·¸ ìˆ˜ì§‘:* {"âœ… í™œì„±í™” (Kubernetes)" if K8S_ENABLED else "âŒ ë¹„í™œì„±í™”"}
"""


@app.event("message")
def handle_message_events(body, logger):
    """ì¼ë°˜ ë©”ì‹œì§€ ì´ë²¤íŠ¸"""
    logger.debug(f"ğŸ“¨ ë©”ì‹œì§€ ì´ë²¤íŠ¸ ìˆ˜ì‹ : {body}")


# ëª¨ë“  ì´ë²¤íŠ¸ ë¡œê¹… (ë””ë²„ê·¸ìš©)
@app.event(".*")
def handle_all_events(event, logger):
    """ëª¨ë“  ì´ë²¤íŠ¸ ë¡œê¹… (ë””ë²„ê·¸ìš©)"""
    event_type = event.get('type', 'unknown')
    logger.info(f"ğŸ”” ì´ë²¤íŠ¸ ìˆ˜ì‹ : {event_type}")
    if event_type == 'app_mention':
        logger.info(f"ğŸ¯ ë©˜ì…˜ ì´ë²¤íŠ¸ ìƒì„¸: {event}")


# ===== ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • =====

def run_scheduler():
    """ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰"""
    # 1ë¶„ë§ˆë‹¤ í—¬ìŠ¤ì²´í¬
    schedule.every(1).minutes.do(monitor_health)
    
    # 5ë¶„ë§ˆë‹¤ ë¦¬ì†ŒìŠ¤ ì²´í¬
    schedule.every(5).minutes.do(monitor_resources)
    
    logger.info("Scheduler started")
    
    while True:
        schedule.run_pending()
        time.sleep(1)


# ===== ë©”ì¸ ì‹¤í–‰ =====

if __name__ == "__main__":
    import threading
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
    scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
    scheduler_thread.start()
    
    # Socket Mode Handler ì‹œì‘
    handler = SocketModeHandler(app, secrets['SLACK_APP_TOKEN'])
    
    logger.info("âš¡ï¸ Slack Monitoring Bot starting in Socket Mode...")
    logger.info(f"Alert Channel: {ALERT_CHANNEL}")
    logger.info(f"Monitoring Services: {list(SERVICES.keys())}")
    logger.info(f"Thresholds - Memory: {MEMORY_THRESHOLD_PCT}% (percentage-based), Error Rate: {ERROR_RATE_THRESHOLD}%, Response Time: {RESPONSE_TIME_THRESHOLD}ms")
    logger.info(f"AI Analysis: {'Enabled (Bedrock)' if AI_ANALYSIS_ENABLED else 'Disabled'}")
    logger.info(f"Log Collection: {'Enabled (Kubernetes)' if K8S_ENABLED else 'Disabled'}")
    
    # ì‹œì‘ ì•Œë¦¼
    start_message = "ëª¨ë‹ˆí„°ë§ ë´‡ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    if AI_ANALYSIS_ENABLED:
        start_message += " (AI ì—ëŸ¬ ë¶„ì„ í™œì„±í™”)"
    if K8S_ENABLED:
        start_message += " (ë¡œê·¸ ìˆ˜ì§‘ í™œì„±í™”)"
    send_alert(start_message, severity="success")
    
    handler.start()
